{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92b5b67-73c8-4e49-9f76-f24111da265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement des ressources NLTK...\n",
      "Ressources téléchargées avec succès!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Télécharger les ressources NLTK\n",
    "print(\"Téléchargement des ressources NLTK...\")\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"Ressources téléchargées avec succès!\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626e7ce6-d9d9-45f4-8ca1-da428575062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration des chemains d'acces vers les dossier des images\n",
    "\n",
    "DATA_DIR = '../dataset'\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, 'train2017')      \n",
    "VAL_IMAGES_DIR = os.path.join(DATA_DIR, 'val2017')         \n",
    "ANNOTATIONS_FILE = os.path.join(DATA_DIR, 'annotations', 'instances_train2017.json')  #fichier qui contient les details et captions des images\n",
    "\n",
    "\n",
    "#parametre : nouvelle taille des images\n",
    "\n",
    "TARGET_SIZE = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8936eeb-61bd-4ab9-8349-49baa7bef58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage et de tokenisation des captions\n",
    "\n",
    "def clean_caption(caption): \n",
    "\n",
    "    caption = caption.lower()                                #minuscule\n",
    "    caption = re.sub(r'[^a-z0-9\\s]', '', caption)            #supression des caracteres non aplhanumeriques\n",
    "    caption = re.sub(r'\\s+', ' ', caption).strip()           #remplacement de multiples espaces en un seul\n",
    "\n",
    "\n",
    "    return caption\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e7d4860-a8fe-4224-a517-97aa202f78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de pretraitement des images\n",
    "\n",
    "def preprocess_image(filepath, target_size=TARGET_SIZE):    \n",
    "\n",
    "    try: \n",
    "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)                             #lire en echelle de gris\n",
    "        \n",
    "        if img is None: \n",
    "           print(f\"Erreur de chargement de l'image : {filepath}\")\n",
    "           return None\n",
    "\n",
    "        resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return resized_img\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de l'image {filepath}: {e}\")\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef928cd-fe8d-4237-b984-34839619d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du vocabulaire\n",
    "def process_captions(annotations_file):\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        #lire le fichier json\n",
    "        coco_json = json.load(f)\n",
    "    \n",
    "    category_dict = {cat['id']: cat['name'] for cat in coco_json['categories']}          #dictionnaire : cat-id : cat-name\n",
    "    \n",
    "    image_data = {}                                                                      #dictionnaire : img-id : caption\n",
    "    for img_meta in coco_json['images']:\n",
    "        image_data[img_meta['id']] = {\n",
    "            'filename': img_meta['file_name'],\n",
    "            'cleaned_captions': []\n",
    "        }\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for annotation in coco_json['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id']\n",
    "        \n",
    "        # Recuperer le nom de la catégorie\n",
    "        raw_caption = category_dict.get(category_id, \"unknown\")\n",
    "        cleaned_caption = clean_caption(raw_caption)\n",
    "        \n",
    "        # Vérifier si la légende nettoyée existe déjà pour cette image\n",
    "        if cleaned_caption and cleaned_caption not in image_data[image_id]['cleaned_captions']:\n",
    "            image_data[image_id]['cleaned_captions'].append(cleaned_caption)\n",
    "            all_words.append(cleaned_caption)\n",
    "    \n",
    "    # Reformatage en DataFrame pour une manipulation facile (en dehors de la boucle!)\n",
    "    df_captions = pd.DataFrame([\n",
    "        {\n",
    "            'image_id': img_id,\n",
    "            'filename': data['filename'],\n",
    "            'captions': data['cleaned_captions']  # ← Suffit largement !\n",
    "        }\n",
    "        for img_id, data in image_data.items()\n",
    "        if data['cleaned_captions']\n",
    "    ])\n",
    "    \n",
    "    print(f\"Nombre total d'images avec légendes : {len(df_captions)}\")\n",
    "    print(f\"Nombre total de tokens (avant filtrage du vocabulaire) : {len(all_words)}\")\n",
    "    \n",
    "    # Construction d'un vocabulaire filtré (Liste de tous les mots uniques)\n",
    "    vocabulary = sorted(list(set(all_words)))\n",
    "    print(f\"Taille du vocabulaire brut (mots uniques) : {len(vocabulary)}\")\n",
    "    \n",
    "    return df_captions, vocabulary              #vocabulary est une liste des categories differentes\n",
    "                                                #df_captions est le dataset a utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac173c5-4ecf-4bb8-96db-5a3bdfd36b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'images avec légendes : 5000\n",
      "Nombre total de tokens (avant filtrage du vocabulaire) : 14569\n",
      "Taille du vocabulaire brut (mots uniques) : 80\n",
      "\n",
      "--- Résultat du Prétraitement Textuel ---\n",
      "   image_id          filename  \\\n",
      "0      5802  000000005802.jpg   \n",
      "1     61181  000000061181.jpg   \n",
      "2     86320  000000086320.jpg   \n",
      "3    463836  000000463836.jpg   \n",
      "4     52759  000000052759.jpg   \n",
      "\n",
      "                                            captions  \n",
      "0       [bottle, person, knife, bowl, cup, backpack]  \n",
      "1  [bicycle, motorcycle, bus, truck, traffic ligh...  \n",
      "2                                             [sink]  \n",
      "3                         [person, backpack, toilet]  \n",
      "4                          [airplane, person, truck]  \n",
      "\n",
      "--- vocabulaire : ---\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat', 'baseball glove', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'cell phone', 'chair', 'clock', 'couch', 'cow', 'cup', 'dining table', 'dog', 'donut', 'elephant', 'fire hydrant', 'fork', 'frisbee', 'giraffe', 'hair drier', 'handbag', 'horse', 'hot dog', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'parking meter', 'person', 'pizza', 'potted plant', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'spoon', 'sports ball', 'stop sign', 'suitcase', 'surfboard', 'teddy bear', 'tennis racket', 'tie', 'toaster', 'toilet', 'toothbrush', 'traffic light', 'train', 'truck', 'tv', 'umbrella', 'vase', 'wine glass', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "# --- EXÉCUTION DU PRÉTRAITEMENT TEXTUEL ---\n",
    "df_train_captions, train_vocabulary = process_captions(ANNOTATIONS_FILE)\n",
    "\n",
    "\n",
    "print(\"\\n--- Résultat du Prétraitement Textuel ---\")\n",
    "print(df_train_captions.head())\n",
    "\n",
    "print(\"\\n--- vocabulaire : ---\")\n",
    "print(train_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f5566cf-51b2-4640-acc3-052c964280e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données sauvegardées en JSON !\n"
     ]
    }
   ],
   "source": [
    "#sauvegarde du dataframe traite et le vocabulaire pour utilisation dans les prochaines etapes\n",
    "\n",
    "#1) dataframe\n",
    "df_train_captions.to_json('df_train_captions.json', orient='records', indent=2)\n",
    "\n",
    "# 2) vocabulaire\n",
    "with open('train_vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_vocabulary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Données sauvegardées en JSON !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0c95a-f19f-44b8-886a-daa88f623d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
